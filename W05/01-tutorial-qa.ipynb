{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "id": "css_setup",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from IPython.core.display import HTML\n",
        "HTML(f\"\"\"\n",
        "<style>\n",
        "</style>\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OVh8wbajC-P3vl4xviSNw",
      "metadata": {},
      "source": [
        "# Experimenting with the tutorial\n",
        "\n",
        "**Overview**\n",
        "This exercise is related to the week 4 tutorial.\n",
        " \n",
        "The individual tasks will ask you to either reflect on parts of the tutorial or modify specific code cells from the tutorial. Specifically, [Task 2](#project) and [Task 3](#ls) require modifications to the code of the tutorial notebook.\n",
        " \n",
        "\n",
        "\n",
        "<article class=\"message\">\n",
        "    <div class=\"message-body\">\n",
        "        <strong>List of tasks</strong>\n",
        "        <ul style=\"list-style: none;\">\n",
        "            <li>\n",
        "            <a href=\"#copy\">Task 1: Copy notebook</a>\n",
        "            </li>\n",
        "            <li>\n",
        "            <a href=\"#project\">Task 2: Projection experiments</a>\n",
        "            </li>\n",
        "            <li>\n",
        "            <a href=\"#ls\">Task 3: Linear Least Squares Experiments</a>\n",
        "            </li>\n",
        "            <li>\n",
        "            <a href=\"#ls1\">Task 4: Linear Least Squares Reflections</a>\n",
        "            </li>\n",
        "            <li>\n",
        "            <a href=\"#poly\">Task 5: Second-order polynomial</a>\n",
        "            </li>\n",
        "            <li>\n",
        "            <a href=\"#pmatrix\">Task 6: Projection matrix</a>\n",
        "            </li>\n",
        "        </ul>\n",
        "    </div>\n",
        "</article>\n",
        "\n",
        "\n",
        "---\n",
        "**Task 1 (easy): Copy notebook**\n",
        "Copy the tutorial notebook\n",
        " in the repository. \n",
        "This makes it easy to go back to the original in case something goes wrong.\n",
        "\n",
        "---\n",
        "---\n",
        "**Task 2 (easy): Projection experimentsüë©‚ÄçüíªüìΩÔ∏è**\n",
        "This task builds on the $\\textbf{Projections}$ section in the tutorial.\n",
        "1. Search and identify comment `##1`\n",
        ". \n",
        "2. Change the values of the matrix $A$ (below comment `##1`\n",
        ") to modify the line. Experiment with different values and observe how the projection changes in the plot.\n",
        "3. Change the matrix $A$, such that $PX$ ‚âà $X$ (that is the projection leaves $X$ almost unchanged). \n",
        "4. Search and identify comment `##2`\n",
        ".\n",
        "5. Set the matrix $A$  = $\\begin{bmatrix} 1 \\\\ 0.5 \\end{bmatrix}$, then apply the projection matrix $P$ twice, i.e. calculate $PPX$ (just below the comment). How does this affect the projected points?\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d7903b",
      "metadata": {},
      "source": [
        "# task 2 (me trying to understand anything?)\n",
        "1. the Matrix A is a vector. It has the length 1 and 0.5. Itis the one that we project with P after. \n",
        "2. When doing this we can see that the values change a little. Depending on what values you do. If you changed it to [1,0] it is a \"vandret\" linje, and therefore not very close to the original point. (OBS. Remember this is a vector, and should therefore be seen as to start at the origin (0,0) of the coordinate system.)\n",
        "3. I guessed around, and found the closest value to be around 0.75, now that this hits the middle point. (This was done by eyeballing the visualisation above the task, and changing the vector according to this.)\n",
        "4. Found it\n",
        "5. When doing this, it can be seen that this is the exact same value as PX.\n",
        "$$\n",
        "PX = PPX\n",
        "$$\n",
        "Which also makes sense. If you give $P$  a list of points $PX$ that are points that already lay on the line, then the closest point to them is on the line itself."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rGo8tlv33HkjpTHfJ0fMl",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "**Task 3 (easy): Linear Least Squares Experimentsüë©‚Äçüíª**\n",
        "This task builds on the $\\textbf{Linear Least Squares}$ section in the tutorial.\n",
        "1. Search and identify comment `##3`\n",
        ".\n",
        "2. Change the values of the first point in the matrix `X`\n",
        " such that it gradually moves further and further away from the line. Observe how it affects the error $RMS$.\n",
        "3. Add two points to `X`\n",
        " and observe how they affect the fitted line and the error.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2796c66",
      "metadata": {},
      "source": [
        "# Task 3 (me gradually understanding)\n",
        "1. Found\n",
        "2. First it can get huge error marks, but i observed that if it gets long enough away, than it was not as bad.\n",
        "3. I put some wild pictures points in a circle almost, and the error margin was bad, and it was basiccaly in the middle of the circle.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yPqOu_TUBLDtm7cwFn99M",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "**Task 4 (easy): Linear Least Squares Reflectionsüí°üìΩÔ∏è**\n",
        "1. How can you change the two additional points so the fitted line does not move?\n",
        "2. What happens to the error when removing all but two points from `X`\n",
        "?\n",
        "3. What happens when you remove all but one point from `X`\n",
        "?\n",
        "4. Reflect on how the quality of the data affects the projection and thus the solution. \n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aae9e56d",
      "metadata": {},
      "source": [
        "# Task 4\n",
        "1. Add two lines that lay on the same fitted line. This will ensure that it does not change.\n",
        "2. The RMS becomes zero. The vecctor that is the difference between the original point and the projected point becomes zero for both points.3\n",
        "3. Then it does not work. Python gives an error. In y = mx + b, there are 2 unknowns, where the x and yx, is taken from the point and inputted. However now that we have 2 unknowns, we need at least 2 points.\n",
        "4. Chat: \"The quality of the data directly impacts the accuracy of the fitted line. If the data is noisy, sparse, or contains outliers, the projection onto the model space becomes less reliable, leading to a poor or biased solution. High-quality, well-distributed data produces a more stable and accurate fit.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ibh1VUhqJpQLZrjk3ogh",
      "metadata": {},
      "source": [
        "## Pen and paper exercises\n",
        "A 2. order polynomial is given by \n",
        "\n",
        "$$f(x) = w_0 + w_1x + w_2x^2 = \\sum^2_{i=0} w_ix^i.$$\n",
        "\n",
        "Generally, an $N$'th order  polynomial is given by\n",
        "\n",
        "$$f(x) = \\sum^N_{i=0} w_ix^i,$$\n",
        "where $\\mathbb{w}$ is a vector of coefficients.\n",
        "\n",
        "---\n",
        "**Task 5 (medium): Second-order polynomial‚ôæÔ∏èüìΩÔ∏è**\n",
        "1. Identify the knowns and unknowns in the polynomial above.\n",
        "2. Is the function linear or non-linear in $\\mathbb{w}$?\n",
        "3. Is the function linear or non-linear in $\\mathbb{x}$?\n",
        "4. Provide the outline of an algorithm for fitting a second-order polynomial using linear least squares.\n",
        "5. Generalize this algorithm to n-th order polynomials.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1603ed27",
      "metadata": {},
      "source": [
        "# Task 5\n",
        "1. Knowns: The input variable x and the polynomial order (2).           Unknowns: The coefficients w=[$w_0$,$w_1$,$w_2$].\n",
        "2. The function is linear in $w$ because each coefficient appears to the first power and is not multiplied by another coefficient. If 2 w's would be timed together at some place, that it is not linear in w. \n",
        "3. Non-linear, due to the fact that the arguments are not in the first power, which means the relationship between $x$ and $f(x)$ is curved rather than a straight line.\n",
        "4. Step 1: Prepare the data\n",
        "Collect your input data points $(x_i, y_i)$ for $i = 1, \\dots, n$.\n",
        "\n",
        "Step 2: Build the design matrix $X$\n",
        "For a second-order polynomial, each row is $[1,\\ x_i,\\ x_i^2]$.\n",
        "Thus $X \\in \\mathbb{R}^{n \\times 3}$:\n",
        "\n",
        "$X = \\begin{bmatrix}\n",
        "1 & x_1 & x_1^2 \\\\\n",
        "1 & x_2 & x_2^2 \\\\\n",
        "\\vdots & \\vdots & \\vdots \\\\\n",
        "1 & x_n & x_n^2\n",
        "\\end{bmatrix}$\n",
        "\n",
        "Step 3: Formulate the normal equations\n",
        "$\\hat{\\mathbf{w}} = (X^\\top X)^{-1} X^\\top \\mathbf{y}$.\n",
        "\n",
        "What is $\\mathbf{y}$?\n",
        "$\\mathbf{y}$ is the vector of observed outputs from your dataset, formed by stacking the $y_i$ values:\n",
        "$\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$.\n",
        "\n",
        "You are estimating $\\mathbf{w} = [w_0, w_1, w_2]^\\top$ so that the predictions $X\\mathbf{w}$ are as close as possible to $\\mathbf{y}$ in the least-squares sense (i.e., minimizing $|,\\mathbf{y} - X\\mathbf{w},|_2^2$).\n",
        "\n",
        "Step 4: Compute $\\hat{\\mathbf{w}}$\n",
        "Solve for $\\hat{\\mathbf{w}} = [\\hat{w}_0, \\hat{w}_1, \\hat{w}_2]^\\top$ using the formula above.\n",
        "\n",
        "Step 5: Predict\n",
        "Use $\\hat{\\mathbf{w}}$ to compute predictions:\n",
        "$\\hat{f}(x) = \\hat{w}_0 + \\hat{w}_1 x + \\hat{w}_2 x^2$\n",
        "\n",
        "5. Step 1: Prepare the data\n",
        "Collect your input data points $(x_i, y_i)$ for $i = 1, \\dots, n$.\n",
        "\n",
        "Step 2: Build the design matrix $X$\n",
        "For a n'th-order polynomial, each row is $[1,\\ x_i,\\ x_i^2,\\ ... ,\\ x_i^n]$.\n",
        "Thus $X \\in \\mathbb{R}^{n \\times 3}$:\n",
        "\n",
        "$X = \\begin{bmatrix}\n",
        "1 & x_1 & x_1^2 & ... & x_1^n \\\\\n",
        "1 & x_2 & x_2^2 & ... & x_2^n \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
        "1 & x_n & x_n^2 & ... & x_n^n\n",
        "\\end{bmatrix}$\n",
        "\n",
        "Step 3: Formulate the normal equations\n",
        "$\\hat{\\mathbf{w}} = (X^\\top X)^{-1} X^\\top \\mathbf{y}$.\n",
        "\n",
        "What is $\\mathbf{y}$?\n",
        "$\\mathbf{y}$ is the vector of observed outputs from your dataset, formed by stacking the $y_i$ values:\n",
        "$\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$.\n",
        "\n",
        "You are estimating $\\mathbf{w} = [w_0, w_1, w_2, ..., w_n]^\\top$ so that the predictions $X\\mathbf{w}$ are as close as possible to $\\mathbf{y}$ in the least-squares sense (i.e., minimizing $|,\\mathbf{y} - X\\mathbf{w},|_2^2$).\n",
        "\n",
        "Step 4: Compute $\\hat{\\mathbf{w}}$\n",
        "Solve for $\\hat{\\mathbf{w}} = [\\hat{w}_0, \\hat{w}_1, \\hat{w}_2, ..., \\hat{w}_n]^\\top$ using the formula above.\n",
        "\n",
        "Step 5: Predict\n",
        "Use $\\hat{\\mathbf{w}}$ to compute predictions:\n",
        "$\\hat{f}(x) = \\hat{w}_0 + \\hat{w}_1 x + \\hat{w}_2 x^2 + ... + \\hat{w}_n x^n$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "387cf53c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X transposed is: \n",
            " [[ 1  1  1]\n",
            " [ 4  5  1]\n",
            " [16 25  1]]\n",
            "X transposed and X is: \n",
            " [[  3  10  42]\n",
            " [ 10  42 190]\n",
            " [ 42 190 882]]\n",
            "inverse of (X transposed and X) is: \n",
            " [[ 6.55555556 -5.83333333  0.94444444]\n",
            " [-5.83333333  6.125      -1.04166667]\n",
            " [ 0.94444444 -1.04166667  0.18055556]]\n",
            "(X^T X)^-1 X^T [[-1.66666667  1.          1.66666667]\n",
            " [ 2.         -1.25       -0.75      ]\n",
            " [-0.33333333  0.25        0.08333333]]\n",
            "w^hat is:  [-2.33333333  5.         -0.66666667]\n"
          ]
        }
      ],
      "source": [
        "# This section i try to find the inverse of a matrix, because \n",
        "# I cannot do it in hand, because im stupid apprently\n",
        "\n",
        "# This is to genereate the to fit a second order polynomial to 3 points. \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "_X = np.array([\n",
        "    [1,4,16],\n",
        "    [1,5,25],\n",
        "    [1,1,1]\n",
        "])\n",
        "y = np.array([7, 6, 2])\n",
        "_X_tranposed = _X.T\n",
        "\n",
        "print(\"X transposed is: \\n\",_X_tranposed)\n",
        "\n",
        "# Here i try to find the product of x transposed and x\n",
        "XT_T = _X_tranposed @ _X\n",
        "print(\"X transposed and X is: \\n\",XT_T)\n",
        "\n",
        "# the inverse of the above is:\n",
        "inverse = np.linalg.inv(XT_T)\n",
        "print(\"inverse of (X transposed and X) is: \\n\", inverse)\n",
        "\n",
        "# (X^T X)^-1 X^T\n",
        "inverse_with_X_Transposed = inverse @ _X_tranposed\n",
        "print(\"(X^T X)^-1 X^T\", inverse_with_X_Transposed)\n",
        "\n",
        "# At last, we now put the y value on:\n",
        "w_hat = inverse_with_X_Transposed @ y\n",
        "print(\"w^hat is: \", w_hat)\n",
        "\n",
        "# f(x) = -0,66667x^2 + 5x -2,33333"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h7lLBKSuVPjA2wULJXpjf",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "**Task 6 (medium): Projection matrix‚ôæÔ∏è**\n",
        "The projection matrix $P = A(A^\\top A )^{-1}A^\\top$ is, under certain conditions, equal to the identity matrix.\n",
        "1. Give an example of a design matrix $A$ for which $P=I$.\n",
        "2. Explain why projection matrices are usually not identity matrices.\n",
        "3. (optional) Prove a condition for which $P=I$. Hint: when is $A^\\top A=I$?.\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6BAPeXhP8vL3hcRFOU4Dz",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X transposed is: \n",
            " [[1. 0.]\n",
            " [0. 1.]]\n",
            "X transposed and X is: \n",
            " [[1. 0.]\n",
            " [0. 1.]]\n",
            "inverse of (X transposed and X) is: \n",
            " [[1. 0.]\n",
            " [0. 1.]]\n",
            "(X^T X)^-1 X^T, i.e. P: \n",
            " [[1. 0.]\n",
            " [0. 1.]]\n",
            "w^hat is:  [0. 1.]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "_X = np.array([\n",
        "    [1.0, 0.0],\n",
        "    [0.0, 1.0]\n",
        "\n",
        "])\n",
        "y = np.array([0.0, 1.0])\n",
        "_X_tranposed = _X.T\n",
        "\n",
        "print(\"X transposed is: \\n\",_X_tranposed)\n",
        "\n",
        "# Here i try to find the product of x transposed and x\n",
        "XT_T = _X_tranposed @ _X\n",
        "print(\"X transposed and X is: \\n\",XT_T)\n",
        "\n",
        "# the inverse of the above is:\n",
        "inverse = np.linalg.inv(XT_T)\n",
        "print(\"inverse of (X transposed and X) is: \\n\", inverse)\n",
        "\n",
        "# (X^T X)^-1 X^T\n",
        "inverse_with_X_Transposed = inverse @ _X_tranposed\n",
        "print(\"(X^T X)^-1 X^T, i.e. P: \\n\", inverse_with_X_Transposed)\n",
        "\n",
        "# At last, we now put the y value on:\n",
        "w_hat = inverse_with_X_Transposed @ y\n",
        "print(\"w^hat is: \", w_hat)\n",
        "\n",
        "# f(x) = -0,66667x^2 + 5x -2,33333"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74c61072",
      "metadata": {},
      "source": [
        "# 2\n",
        "Projection matrices are usually not identity matrices because they reduce the dimensionality of vectors by removing components orthogonal to the target subspace. The identity matrix only occurs when the projection is onto the entire space.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Wr6PTMlyMjIc5BKie1td_",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "iml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
